# https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus
    environment: development
    tier: monitoring
  name: prometheus
  namespace: default
  annotations:
    prometheus.io/scrape: "true"
spec:
  selector:
    matchLabels:
      app: prometheus
      environment: development
      tier: monitoring
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: prometheus
        environment: development
        tier: monitoring
    spec:
# Added this into prometheus deployment, because we got a error that said prometheus couldn’t create a TSDB with write access. So, the issue could be PVC where user in prometheus container couldn’t have access to PVC. One solution was run prometheus as root user. This is another solution.
      initContainers:
      - name: volume-permissions
        image: busybox
        command: ["sh", "-c", "chown -R 65534:65534 /prometheus && chmod -R 777 /prometheus"]
        volumeMounts:
        - name: prometheus-db
          mountPath: /prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.55.0
        imagePullPolicy: IfNotPresent
        args: ["--config.file=/etc/prometheus/prometheus.yml", "--storage.tsdb.path=/prometheus", "--web.enable-lifecycle"]
        ports:
        - containerPort: 9090
          name: prometheus-port
        volumeMounts:
        - name: prometheus-db
          mountPath: /prometheus
        - name: prometheus-configuration
          mountPath: /etc/prometheus/prometheus.yml
          subPath: prometheus.yml
        - name: rules-config
          mountPath: /etc/prometheus/rules.yaml
          subPath: rules.yaml
      volumes:
        - name: prometheus-db
          persistentVolumeClaim:
            claimName: prometheus-db
          # hostPath:
          #   path: /mnt/disks/prometheus-db # this mounts the filesystem from host to pod
        - name: prometheus-configuration
          configMap:
            name: prometheus-configuration
        - name: rules-config
          configMap:
            name: rules-config
      restartPolicy: Always
      serviceAccountName: prometheus
---

# https://kubernetes.io/docs/concepts/services-networking/service/
apiVersion: v1
kind: Service
metadata:
  name: prometheus-svc
  namespace: default
spec:
  selector:
    app: prometheus
    environment: development
    tier: monitoring
  type: NodePort
  ports:
  - name: wordpress
    protocol: TCP
    port: 9090 #service port
    targetPort: 9090 #container port
    nodePort: 30002 #host port
---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
  - apiGroups: [""]
    resources: ["pods", "endpoints", "services", "nodes"] #added nodes because job for role:node was shown in config page but didn't show up in targets page
    verbs: ["get", "list", "watch"]
#Below permissions are needed for the prometheus service account to scrape metrics wrt to kubernetes-nodes job
  - apiGroups: [""]
    resources: ["nodes/proxy"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["nodes/metrics"]
    verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: default
---
# https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alert-manager
  namespace: default
  labels:
    app: alerting
spec:
  selector:
    matchLabels:
      app: alerting
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: alerting
    spec:
      containers:
      - name: alert-manager
        image: prom/alertmanager:v0.27.0
        imagePullPolicy: IfNotPresent
        # env:
        # - name: ACCEPT_EULA
        #   value: "Y"
        # - name: DB_HOST
        #   valueFrom:
        #     configMapKeyRef:
        #       name: myjob
        #       key: DB_HOST
        ports:
        - containerPort: 9093
          name: alert-manager
        volumeMounts:
        - name: alert-config
          mountPath: /etc/alertmanager/alertmanager.yml
          subPath: alertmanager.yml
      volumes:
      - name: alert-config
        configMap:
          name: alert-config
      restartPolicy: Always
---
# https://kubernetes.io/docs/concepts/services-networking/service/
apiVersion: v1
kind: Service
metadata:
  name: alert-manager
  namespace: default
spec:
  selector:
    app: alerting
  type: NodePort
  ports:
  - name: alert-manager
    protocol: TCP
    port: 9093
    targetPort: 9093
    nodePort: 30004